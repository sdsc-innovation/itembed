{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with word2vec\n",
    "\n",
    "This notebook evaluates both _itembed_ and _word2vec_ on a simple classification task. This is by no mean a definitive conclusion of which method is better-suited for this kind of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itembed import (\n",
    "    pack_itemsets,\n",
    "    prune_itemsets,\n",
    "    initialize_syn,\n",
    "    CompoundTask,\n",
    "    UnsupervisedTask,\n",
    "    SupervisedTask,\n",
    "    train,\n",
    "    softmax,\n",
    "    normalize,\n",
    ")\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from bokeh.plotting import ColumnDataSource, figure, show\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6602</td>\n",
       "      <td>jamaican</td>\n",
       "      <td>[plain flour, sugar, butter, eggs, fresh ginge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42779</td>\n",
       "      <td>spanish</td>\n",
       "      <td>[olive oil, salt, medium shrimp, pepper, garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3735</td>\n",
       "      <td>italian</td>\n",
       "      <td>[sugar, pistachio nuts, white almond bark, flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16903</td>\n",
       "      <td>mexican</td>\n",
       "      <td>[olive oil, purple onion, fresh pineapple, por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12734</td>\n",
       "      <td>italian</td>\n",
       "      <td>[chopped tomatoes, fresh basil, garlic, extra-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...\n",
       "5   6602     jamaican  [plain flour, sugar, butter, eggs, fresh ginge...\n",
       "6  42779      spanish  [olive oil, salt, medium shrimp, pepper, garli...\n",
       "7   3735      italian  [sugar, pistachio nuts, white almond bark, flo...\n",
       "8  16903      mexican  [olive oil, purple onion, fresh pineapple, por...\n",
       "9  12734      italian  [chopped tomatoes, fresh basil, garlic, extra-..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw dataset\n",
    "recipe_df = pd.read_json(\"recipe_train.json\")\n",
    "recipe_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ingredients as a list of list of string\n",
    "itemsets = recipe_df[\"ingredients\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack itemsets into contiguous arrays\n",
    "labels, indices, offsets = pack_itemsets(itemsets, min_count=min_count, min_length=2)\n",
    "num_label = len(labels)\n",
    "label_map = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both methods will use the same configuration, when applicable\n",
    "num_dimension = 64\n",
    "min_count = 5\n",
    "num_negative = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to display scatter plot\n",
    "def plot(syn):\n",
    "    \n",
    "    # Project with UMAP, using cosine similarity measure\n",
    "    model = umap.UMAP(metric='cosine', verbose=1)\n",
    "    projection = model.fit_transform(syn)\n",
    "    \n",
    "    # Pack as a Bokeh data source\n",
    "    source = ColumnDataSource(data=dict(\n",
    "        x=projection[:, 0],\n",
    "        y=projection[:, 1],\n",
    "        label=labels,\n",
    "    ))\n",
    "\n",
    "    # Create plot\n",
    "    p = figure(\n",
    "        width=900,\n",
    "        height=600,\n",
    "        tooltips=[\n",
    "            ('label', '@label'),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Draw tags as points\n",
    "    p.scatter(\n",
    "        'x', 'y',\n",
    "        source=source,\n",
    "    )\n",
    "\n",
    "    # Show in notebook\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch in itembed is 428275 pairs\n",
      "One epoch in word2vec is 4963448 pairs\n",
      "Hence, itembed needs 11.6 times more epochs, i.e. 5 vs 57\n"
     ]
    }
   ],
   "source": [
    "# As word2vec does not sample pairs the same way, we need to estimate the equivalent number of epochs\n",
    "lengths = np.array([len(itemset) for itemset in itemsets])\n",
    "\n",
    "# itembed does a linear number of pairs per itemset (i.e. one per item)\n",
    "itembed_pair_count = lengths.sum()\n",
    "print(f\"One epoch in itembed is {itembed_pair_count} pairs\")\n",
    "\n",
    "# word2vec does a quadratic number of pairs per itemset (i.e. all of them)\n",
    "word2vec_pair_count = (lengths * (lengths - 1)).sum()\n",
    "print(f\"One epoch in word2vec is {word2vec_pair_count} pairs\")\n",
    "\n",
    "# Compute epoch count\n",
    "factor = word2vec_pair_count / itembed_pair_count\n",
    "num_epochs_word2vec = 5\n",
    "num_epochs_itembed = int(num_epochs_word2vec * factor)\n",
    "print(f\"Hence, itembed needs {factor:.1f} times more epochs, i.e. {num_epochs_word2vec} vs {num_epochs_itembed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itembed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings sets from uniform distribution\n",
    "syn0 = initialize_syn(num_label, num_dimension)\n",
    "syn1 = initialize_syn(num_label, num_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unsupervised task, i.e. using co-occurrences\n",
    "task = UnsupervisedTask(indices, offsets, syn0, syn1, num_negative=num_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training\n",
    "# Note: due to a different sampling strategy, more epochs than word2vec are needed\n",
    "train(task, num_epoch=num_epochs_itembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both embedding sets are equivalent, just choose one of them\n",
    "itembed_syn = syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show projection\n",
    "plot(itembed_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using \"infinite\" window, \n",
    "model = Word2Vec(\n",
    "    itemsets,\n",
    "    size=num_dimension,\n",
    "    window=999999,\n",
    "    min_count=min_count,\n",
    "    sg=1,\n",
    "    negative=num_negative,\n",
    "    iter=num_epochs_word2vec,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model re-shuffle the vocabulary, so we need to remap\n",
    "syn = np.zeros((num_label, num_dimension), dtype=np.float32)\n",
    "for i, word in enumerate(labels):\n",
    "    vocab = model.wv.vocab.get(word)\n",
    "    if vocab is not None:\n",
    "        index = vocab.index\n",
    "        syn[i] = model.wv.vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the final embeddings\n",
    "word2vec_syn = syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show projection\n",
    "plot(word2vec_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store itemsets as sparse matrix\n",
    "rows = []\n",
    "cols = []\n",
    "for i, itemset in enumerate(itemsets):\n",
    "    for item in itemset:\n",
    "        j = label_map.get(item)\n",
    "        if j is not None:\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "data = [1.0] * len(rows)\n",
    "recipe = csr_matrix((data, (rows, cols)), shape=(len(itemsets), num_label), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean ingredient embedding as formula embedding\n",
    "X = recipe @ word2vec_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cuisine style based on ingredients\n",
    "y = recipe_df[\"cuisine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\berdat\\.conda\\envs\\dev\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Use simple logistic regression\n",
    "clf = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6453964901694574"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO eval on test set\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greek', 'southern_us'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply on both methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
