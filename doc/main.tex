
\documentclass[a4paper,oneside,12pt]{article}

\usepackage{fontspec}
\usepackage{xunicode}

\usepackage{polyglossia}
\setdefaultlanguage{english}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{parskip}

\usepackage[
  xetex,
  unicode,
  breaklinks,
  hyperfootnotes,
  hyperindex,
  bookmarks,
  bookmarksnumbered,
  pdfusetitle,
  colorlinks
]{hyperref}

\usepackage[
  hyperref=true,
  backend=bibtex,
  citestyle=numeric
]{biblatex}
\bibliography{main}
\nocite{*}

\begin{document}

% https://ruder.io/word-embeddings-softmax/index.html


\section{Logistic function}

\begin{equation}
\sigma (x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x}
\end{equation}

\begin{equation}
\sigma (-x) = 1 - \sigma (x)
\end{equation}

\begin{equation}
\frac{\partial}{\partial x} \sigma (x) = \sigma (x) \sigma (-x)
\end{equation}


\section{Softmax formulation}

Let $(a, b)$ a pair of items, where $a \in A$ is the source and $b \in B$ the target. The actual meaning depends on the use case.

% define A, B
% example: skip gram

The conditional probability of observing $b$ given $a$ is defined by a softmax on all possibilities, as it is a regular multi-class task:

% \Pr\left( A \;\middle\vert\; B \right)
\begin{equation}
P(b \mid a ; \mathbf{u}, \mathbf{v}) = \frac{e^{\mathbf{u}_a^T \mathbf{v}_b}}{\sum_{b'} e^{\mathbf{u}_a^T \mathbf{v}_{b'}}}
\end{equation}

Negative log-likelihood:

\begin{equation}
\mathcal{L} (a, b ; \mathbf{u}, \mathbf{v}) = -\log P(b \mid a ; \mathbf{u}, \mathbf{v}) = -\mathbf{u}_a^T \mathbf{v}_b + \log \sum_{b'} e^{\mathbf{u}_a^T \mathbf{v}_{b'}}
\end{equation}

\begin{equation}
\frac{\partial}{\partial \mathbf{u}_a} \mathcal{L} (a, b ; \mathbf{u}, \mathbf{v}) = -\mathbf{v}_b + \sum_{b'} P(b' \mid a ; \mathbf{u}, \mathbf{v}) \mathbf{v}_{b'}
\end{equation}


\section{Noise contrastive estimation formulation}

Noise Contrastive Estimation (Gutmann and Hyv√§rinen \cite{pmlr-v9-gutmann10a}) is proposed by Mnih and Teh \cite{mnih2012fast} as a stable sampling method, to reduce the cost induced by softmax computation.
In a nutshell, the model is trained to distinguish observed (positive) samples from random noise.
Logistic regression is applied to minimize the negative log-likelihood, i.e. cross-entropy of our training example against the $k$ noise samples:

%\[ P (a, b) = P(y = 1 \mid a, b) \prod_{b' \in B^* \subseteq B} P(y = 0 \mid a, b') \]

\begin{equation}
\mathcal{L} (a, b) = - \log P(y = 1 \mid a, b) + k \mathbb{E}_{b' \sim Q}\left[ - \log P(y = 0 \mid a, b) \right]
\end{equation}

To avoid computating the expectation on the whole vocabulary, a Monte Carlo approximation is applied.
$B^* \subseteq B$, with $\vert B^* \vert = k$, is therefore the set of random samples used to estimate it:

\begin{equation}
\mathcal{L} (a, b) = - \log P(y = 1 \mid a, b) - k \sum_{b' \in B^* \subseteq B} \log P(y = 0 \mid a, b')
\end{equation}

We are effectively generating samples from two different distributions: positive pairs are sampled from the empirical training set, while negative pairs come from the noise distribution $Q$.

\begin{equation}
P(y, b \mid a) = \frac{1}{k + 1} P(b \mid a) + \frac{k}{k + 1} Q(b)
\end{equation}

Hence, the probability that a sample came from the training distribution:

\begin{equation}
P(y = 1 \mid a, b) = \frac{P(b \mid a)}{P(b \mid a) + k Q(b)}
\end{equation}

\begin{equation}
P(y = 0 \mid a, b) = 1 - P(y = 1 \mid a, b)
\end{equation}

However, $P(b \mid a)$ is still defined as a softmax:

\begin{equation}
P(b \mid a ; \mathbf{u}, \mathbf{v}) = \frac{e^{\mathbf{u}_a^T \mathbf{v}_b}}{\sum_{b'} e^{\mathbf{u}_a^T \mathbf{v}_{b'}}}
\end{equation}

Both Mnih and Teh \cite{mnih2012fast} and Vaswani et al. \cite{vaswani-etal-2013-decoding} arbitrarily set the denominator to $1$.
The underlying idea is that, instead of explicitly computing this value, it could be defined as a trainable parameter.
Zoph et al. \cite{zoph-fast} actually report that even when trained, it stays close to $1$ with a low variance.

Hence:

\begin{equation}
P(b \mid a ; \mathbf{u}, \mathbf{v}) = e^{\mathbf{u}_a^T \mathbf{v}_b}
\end{equation}

The negative log-likelihood can then be computed as usual:

\begin{equation}
\mathcal{L} (a, b ; \mathbf{u}, \mathbf{v}) = -\log P (a, b ; \mathbf{u}, \mathbf{v})
\end{equation}

Mnih and Teh \cite{mnih2012fast} report that using $k = 25$ is sufficient to match the performance of the regular softmax.


\section{Negative sampling formulation}

Negative Sampling, popularised by Mikolov et al. \cite{Mikolov2013DistributedRO}, can be seen as an approximation of NCE.
As defined previously, NCE is based on the following:

\begin{equation}
P(y = 1 \mid a, b ; \mathbf{u}, \mathbf{v}) = \frac{e^{\mathbf{u}_a^T \mathbf{v}_b}}{e^{\mathbf{u}_a^T \mathbf{v}_b} + k Q(b)}
\end{equation}

Negative Sampling simplifies this computation by replacing $k Q(b)$ by $1$.
Note that $\k Q(b) = 1$ is true when $B^* = B$ and $Q$ is the uniform distribution.

\begin{equation}
P(y = 1 \mid a, b ; \mathbf{u}, \mathbf{v}) = \frac{e^{\mathbf{u}_a^T \mathbf{v}_b}}{e^{\mathbf{u}_a^T \mathbf{v}_b} + 1} = \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right)
\end{equation}


Hence:

\begin{equation}
P(a, b ; \mathbf{u}, \mathbf{v}) = \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right) \prod_{b' \in B^* \subseteq B} \left( 1 - \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right) \right)
\end{equation}

\begin{equation}
\mathcal{L} (a, b ; \mathbf{u}, \mathbf{v}) = -\log \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right) - \sum_{b' \in B^* \subseteq B} \log \left( 1 - \sigma \left( \mathbf{u}_a^T \mathbf{v}_b' \right) \right)
\end{equation}

%\[ P(y \mid a, b ; \mathbf{u}, \mathbf{v}) = \left( \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right) \right)^y + \left( 1 - \sigma \left( \mathbf{u}_a^T \mathbf{v}_b \right) \right)^{1 - y} \]

For more details, see Goldberg and Levy's notes \cite{Goldberg2014word2vecED}.

To compute the gradient, let us rewrite the loss as:

\begin{equation}
\mathcal{L} (a, b ; \mathbf{u}, \mathbf{v}) = -\ell_{a, b, 1} - \sum_{b' \in B^* \subseteq B} \ell_{a, b', 0}
\end{equation}

where

\begin{equation}
\ell_{a, b, y} = \log \sigma \left( y - \mathbf{u}_a^T \mathbf{v}_b \right)
\end{equation}

Then:

\begin{equation}
\begin{array}{lll}
\frac{\partial}{\partial \mathbf{u}_a} \ell (a, b, y) & = & \frac{1}{y - \sigma \left(\mathbf{u}_a^T \mathbf{v}_b \right)}
\left( - \sigma \left(\mathbf{u}_a^T \mathbf{v}_b \right) \left( 1 - \sigma \left(\mathbf{u}_a^T \mathbf{v}_b \right) \right) \right) \mathbf{v}_b \\
& = & \left( y - \sigma \left(\mathbf{u}_a^T \mathbf{v}_b \right) \right) \mathbf{v}_b
\end{array}
\end{equation}

And similarly:

\begin{equation}
\frac{\partial}{\partial \mathbf{v}_b} \ell (a, b, y) = \left( y - \sigma \left(\mathbf{u}_a^T \mathbf{v}_b \right) \right) \mathbf{u}_a
\end{equation}


\section{Normalization}

By setting the denominator to $1$, as proposed above, the model essentially learns to self-normalize.
However, Devlin et al. \cite{devlin-etal-2014-fast} suggest to add a squared error penalty to enforce the equivalence.
Andreas and Klein \cite{andreas-norm} even suggest that it should even be sufficient to only normalize a fraction of the training examples and still obtain approximate self-normalising behaviour.


\section{Item distribution balancing}

In word2vec, Mikolov et al. \cite{Mikolov2013DistributedRO} use a subsampling approach to reduce the impact of frequent words.
Each word has a probability

\begin{equation}
P(w_i) = 1 - \sqrt{ \left( \frac{t}{f(w_i)} \right) }
\end{equation}

of being discarded, where $f(w_i)$ is its frequency and $t$ a chosen threshold, typically around $10^{-5}$.


\section{Parallelization}

Hogwild \cite{Recht2011HogwildAL}.


\printbibliography


\end{document}
